{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C1_W3_Lab_1_improving_accuracy_using_convolutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swati2697/MLpractice/blob/main/Improving_accuracy_using_convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa"
      },
      "source": [
        "# Improving Computer Vision Accuracy using Convolutions\n",
        "\n",
        "In the previous lessons you saw how to do fashion recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with the impact of different sizes of hidden layer, number of training epochs etc on the final accuracy.\n",
        "\n",
        "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. \n",
        "\n",
        "(**Note:** You can run the notebook using TensorFlow 2.5.0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxiu1SDU36aj"
      },
      "source": [
        "#!pip install tensorflow==2.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcsRtq9OLorS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16575e7-7daf-45a0-b9c3-aea0603d17bb"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFDMnG1X36aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3aba60c-8c86-4d7f-9d60-57b3d4e36007"
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5018 - accuracy: 0.8228\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3778 - accuracy: 0.8624\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3389 - accuracy: 0.8759\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3118 - accuracy: 0.8852\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2964 - accuracy: 0.8899\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3510 - accuracy: 0.8748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zldEXSsF8Noz"
      },
      "source": [
        "Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to details on Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, details. \n",
        "\n",
        "If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar.\n",
        "\n",
        "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\n",
        "\n",
        "This is perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on the highlighted features.\n",
        "\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.\n",
        "\n",
        "Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0tFgT1MMKi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b821fec-bb90-4eaf-928f-11bf561749fa"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=25)\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "1875/1875 [==============================] - 26s 7ms/step - loss: 0.4513 - accuracy: 0.8356\n",
            "Epoch 2/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2984 - accuracy: 0.8913\n",
            "Epoch 3/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2491 - accuracy: 0.9078\n",
            "Epoch 4/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2162 - accuracy: 0.9188\n",
            "Epoch 5/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1914 - accuracy: 0.9288\n",
            "Epoch 6/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1680 - accuracy: 0.9366\n",
            "Epoch 7/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1508 - accuracy: 0.9438\n",
            "Epoch 8/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1310 - accuracy: 0.9507\n",
            "Epoch 9/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1168 - accuracy: 0.9559\n",
            "Epoch 10/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1032 - accuracy: 0.9613\n",
            "Epoch 11/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0913 - accuracy: 0.9659\n",
            "Epoch 12/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0799 - accuracy: 0.9693\n",
            "Epoch 13/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0731 - accuracy: 0.9726\n",
            "Epoch 14/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0651 - accuracy: 0.9755\n",
            "Epoch 15/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0587 - accuracy: 0.9779\n",
            "Epoch 16/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0537 - accuracy: 0.9796\n",
            "Epoch 17/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0485 - accuracy: 0.9818\n",
            "Epoch 18/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0463 - accuracy: 0.9822\n",
            "Epoch 19/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0405 - accuracy: 0.9850\n",
            "Epoch 20/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0404 - accuracy: 0.9850\n",
            "Epoch 21/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0355 - accuracy: 0.9869\n",
            "Epoch 22/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0336 - accuracy: 0.9868\n",
            "Epoch 23/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0317 - accuracy: 0.9886\n",
            "Epoch 24/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0315 - accuracy: 0.9890\n",
            "Epoch 25/25\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0305 - accuracy: 0.9893\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.5784 - accuracy: 0.9113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRLfZ0jt-fQI"
      },
      "source": [
        "It's likely gone up to about 93% on the training data and 91% on the validation data. \n",
        "\n",
        "That's significant, and a step in the right direction!\n",
        "\n",
        "Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. \n",
        "\n",
        "(In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing *other* data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.)\n",
        "\n",
        "Then, look at the code again, and see, step by step how the Convolutions were built:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaLX5cgI_JDb"
      },
      "source": [
        "Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS_W_INc_kJQ"
      },
      "source": [
        "Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\n",
        "\n",
        "1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
        "2. The size of the Convolution, in this case a 3x3 grid\n",
        "3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\n",
        "4. In the first layer, the shape of the input data.\n",
        "\n",
        "You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%.\n",
        "\n",
        "You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. \n",
        "\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMorM6daADjA"
      },
      "source": [
        "Add another convolution\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1-x-kZF4_tC"
      },
      "source": [
        "Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Flatten(),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPtqR23uASjX"
      },
      "source": [
        "The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0GSsjUhAaSj"
      },
      "source": [
        "Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "This code will show us the convolutions graphically. The print (test_labels[:100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-6nX4QsOku6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3040999d-2295-4cc9-fa10-8fb16175e9f7"
      },
      "source": [
        "print(test_labels[:100])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FGsHhv6JvDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "0432101d-c26b-476c-d183-f3b6c9f3989c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=23\n",
        "THIRD_IMAGE=39\n",
        "CONVOLUTION_NUMBER = 1\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVPZqgHo5Ux"
      },
      "source": [
        "### EXERCISES\n",
        "\n",
        "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
        "\n",
        "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
        "\n",
        "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
        "\n",
        "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n",
        "\n",
        "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpYRidBXpBPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32c32dc-695f-4fad-9bf3-3e93ec00c10e"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.1595 - accuracy: 0.9513\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0542 - accuracy: 0.9837\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0348 - accuracy: 0.9889\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0233 - accuracy: 0.9925\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0155 - accuracy: 0.9948\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0118 - accuracy: 0.9963\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0080 - accuracy: 0.9972\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0069 - accuracy: 0.9978\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0056 - accuracy: 0.9983\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0054 - accuracy: 0.9983\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0607 - accuracy: 0.9867\n",
            "0.9866999983787537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 1 - 16 convolutions\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFKRrfKmd1EM",
        "outputId": "cce459c8-6d95-4237-d685-979a3e60ad2b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1686 - accuracy: 0.9497\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0616 - accuracy: 0.9812\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0390 - accuracy: 0.9877\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0273 - accuracy: 0.9912\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0193 - accuracy: 0.9940\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0131 - accuracy: 0.9958\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0105 - accuracy: 0.9965\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0071 - accuracy: 0.9977\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0070 - accuracy: 0.9975\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0047 - accuracy: 0.9985\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0598 - accuracy: 0.9863\n",
            "0.986299991607666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#64 convolutions\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYGKj1EiewEI",
        "outputId": "eec7eb3d-379b-427c-f8a7-9ebbd850115a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1393 - accuracy: 0.9580\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0475 - accuracy: 0.9854\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0284 - accuracy: 0.9913\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0191 - accuracy: 0.9940\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0124 - accuracy: 0.9961\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0086 - accuracy: 0.9973\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0072 - accuracy: 0.9978\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0053 - accuracy: 0.9982\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0050 - accuracy: 0.9985\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0047 - accuracy: 0.9986\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0523 - accuracy: 0.9875\n",
            "0.987500011920929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exercise 2 with 32 convolutions\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  #tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  #tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7_qUFvVf2N6",
        "outputId": "a9c979a9-462e-4278-eefb-c394e823cd37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.4022 - accuracy: 0.8569\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2725 - accuracy: 0.9020\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2269 - accuracy: 0.9173\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1937 - accuracy: 0.9282\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1618 - accuracy: 0.9401\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1387 - accuracy: 0.9481\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1169 - accuracy: 0.9574\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0997 - accuracy: 0.9629\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0833 - accuracy: 0.9697\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0726 - accuracy: 0.9739\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3184 - accuracy: 0.9147\n",
            "0.9146999716758728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exercise 3\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2), #extra convolution layer\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TNzIe8Bg6br",
        "outputId": "9c550399-b138-4cb5-ffb8-8624c059ba8e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.6370 - accuracy: 0.7657\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4408 - accuracy: 0.8378\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3845 - accuracy: 0.8576\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3469 - accuracy: 0.8732\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3211 - accuracy: 0.8813\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3024 - accuracy: 0.8875\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2871 - accuracy: 0.8935\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2730 - accuracy: 0.8994\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2631 - accuracy: 0.9027\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2537 - accuracy: 0.9059\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.3356 - accuracy: 0.8807\n",
            "0.8806999921798706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2), #extra convolution layer\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2), #extra convolution layer\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "Rdbqf1rBiKYt",
        "outputId": "46fcadca-16ce-4bf9-fa35-5f6a14de74bd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bd00a8c83d8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m ])\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       raise ValueError(\n\u001b[0;32m--> 303\u001b[0;31m           \u001b[0;34mf'One of the dimensions in the output is <= 0 '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m           \u001b[0;34mf'due to downsampling in {self.name}. Consider '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m           \u001b[0;34mf'increasing the input size. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d_16. Consider increasing the input size. Received input shape [None, 1, 1, 32] which would produce output shape with a zero or negative value in a dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exercise 5 - 3 convolutions\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.98):\n",
        "      print(\"\\nReached desired accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "      \n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "callbacks = myCallback() #callback\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2), #extra convolution layer\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks = [callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwpMW3axibsz",
        "outputId": "14019f27-2b95-41f6-bddb-58b016636016"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.6458 - accuracy: 0.7628\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4446 - accuracy: 0.8373\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3864 - accuracy: 0.8578\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3524 - accuracy: 0.8716\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3270 - accuracy: 0.8797\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3086 - accuracy: 0.8859\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2917 - accuracy: 0.8929\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2777 - accuracy: 0.8978\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2670 - accuracy: 0.9010\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2554 - accuracy: 0.9041\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.3214 - accuracy: 0.8874\n",
            "0.8873999714851379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 convolutions\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.98):\n",
        "      print(\"\\nReached desired accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "      \n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "callbacks = myCallback() #callback\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks = [callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRIVGv01k83l",
        "outputId": "864b9299-86df-45c5-9d38-7606dcdbeb2d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4743 - accuracy: 0.8279\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3196 - accuracy: 0.8835\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2724 - accuracy: 0.9002\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2421 - accuracy: 0.9093\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2172 - accuracy: 0.9191\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1954 - accuracy: 0.9279\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1788 - accuracy: 0.9323\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1626 - accuracy: 0.9391\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1483 - accuracy: 0.9434\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1357 - accuracy: 0.9485\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2854 - accuracy: 0.9077\n",
            "0.9077000021934509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 convolution\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.98):\n",
        "      print(\"\\nReached desired accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "      \n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "callbacks = myCallback() #callback\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks = [callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Qkf-UAlmk7",
        "outputId": "5bc9bc10-de38-481e-8d75-e43053e876ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.3890 - accuracy: 0.8631\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2638 - accuracy: 0.9055\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2181 - accuracy: 0.9205\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1863 - accuracy: 0.9311\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1583 - accuracy: 0.9410\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1351 - accuracy: 0.9501\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1149 - accuracy: 0.9571\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0981 - accuracy: 0.9649\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0826 - accuracy: 0.9694\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0711 - accuracy: 0.9739\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3164 - accuracy: 0.9141\n",
            "0.9140999913215637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20 epochs\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.98):\n",
        "      print(\"\\nReached desired accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "      \n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "callbacks = myCallback() #callback\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=20, callbacks = [callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N-bsQ4hsCHK",
        "outputId": "765bab4e-ccfb-43a7-bdb2-864eccbb6a95"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.3863 - accuracy: 0.8613\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2631 - accuracy: 0.9050\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2184 - accuracy: 0.9201\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1856 - accuracy: 0.9309\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1578 - accuracy: 0.9414\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1374 - accuracy: 0.9498\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1129 - accuracy: 0.9590\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0971 - accuracy: 0.9634\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0828 - accuracy: 0.9699\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0711 - accuracy: 0.9739\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0596 - accuracy: 0.9785\n",
            "Epoch 12/20\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.0509 - accuracy: 0.9819\n",
            "Reached desired accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0509 - accuracy: 0.9819\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3485 - accuracy: 0.9169\n",
            "0.9168999791145325\n"
          ]
        }
      ]
    }
  ]
}